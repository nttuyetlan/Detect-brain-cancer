{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1l5KB4Clrmi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import re\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "import tiktoken\n",
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "import numpy as np\n",
        "import fitz  # PyMuPDF\n",
        "from typing import List, Tuple\n",
        "\n",
        "import nltk\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Thêm import LangChain\n",
        "\n",
        "# ---------------------------------------\n",
        "# Thiết lập logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='[%(levelname)s] %(asctime)s %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Kiểm tra và tải tokenizer punkt của NLTK nếu cần\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    logging.info(\"NLTK punkt đã có sẵn.\")\n",
        "except LookupError:\n",
        "    logging.info(\"Đang tải NLTK punkt...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    logging.info(\"NLTK punkt tải xong.\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# 1. Load config Azure OpenAI và Milvus từ .env\n",
        "load_dotenv()\n",
        "\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")  # ví dụ \"2024-10-21\"\n",
        "EMBED_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_EMBED\")\n",
        "CHAT_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_CHAT\")\n",
        "\n",
        "if not all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_OPENAI_API_VERSION, EMBED_DEPLOYMENT, CHAT_DEPLOYMENT]):\n",
        "    logging.error(\"Vui lòng kiểm tra biến môi trường Azure OpenAI trong .env\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Khởi tạo Azure OpenAI client\n",
        "client = AzureOpenAI(\n",
        "    api_key=AZURE_OPENAI_KEY,\n",
        "    api_version=AZURE_OPENAI_API_VERSION,\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
        ")\n",
        "\n",
        "# Milvus config\n",
        "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
        "MILVUS_PORT = int(os.getenv(\"MILVUS_PORT\", 19530))\n",
        "\n",
        "# ---------------------------------------\n",
        "# 2. Kết nối Milvus và tạo Collection nếu cần\n",
        "def connect_milvus():\n",
        "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
        "    logging.info(f\"[Milvus] Connected to {MILVUS_HOST}:{MILVUS_PORT}\")\n",
        "\n",
        "def create_collection(collection_name: str, dim: int):\n",
        "    existing = utility.list_collections()\n",
        "    if collection_name in existing:\n",
        "        logging.info(f\"[Milvus] Collection '{collection_name}' đã tồn tại, drop để tạo lại mới.\")\n",
        "        Collection(collection_name).drop()\n",
        "    # Định nghĩa schema\n",
        "    fields = [\n",
        "        FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
        "        FieldSchema(name=\"doc_id\", dtype=DataType.INT64),\n",
        "        FieldSchema(name=\"chunk_id\", dtype=DataType.INT64),\n",
        "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
        "    ]\n",
        "    schema = CollectionSchema(fields, description=\"RAG internal knowledge base\")\n",
        "    collection = Collection(name=collection_name, schema=schema)\n",
        "    # Tạo index cho vector field\n",
        "    index_params = {\n",
        "        \"index_type\": \"IVF_FLAT\",\n",
        "        \"metric_type\": \"IP\",           # dùng IP để tìm cosine similarity khi vector đã normalize\n",
        "        \"params\": {\"nlist\": 128}\n",
        "    }\n",
        "    logging.info(f\"[Milvus] Creating index on '{collection_name}.embedding' ...\")\n",
        "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
        "    collection.load()\n",
        "    logging.info(f\"[Milvus] Collection '{collection_name}' is ready.\")\n",
        "    return collection\n",
        "\n",
        "# ---------------------------------------\n",
        "# 3. Đọc tài liệu nội bộ\n",
        "def load_texts_from_pdf(path: str) -> str:\n",
        "    \"\"\"Đọc text và table từ PDF qua PyMuPDF, table sẽ được prefix [TABLE].\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(path)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Không mở được PDF '{path}': {e}\")\n",
        "        return \"\"\n",
        "    blocks = []\n",
        "    for page in doc:\n",
        "        for b in page.get_text(\"blocks\"):\n",
        "            text = b[4].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            # Nhận diện table đơn giản: có tab hoặc nhiều khoảng trắng liên tiếp\n",
        "            if \"\\t\" in text or re.search(r' {2,}', text):\n",
        "                blocks.append(\"[TABLE]\\n\" + text)\n",
        "            else:\n",
        "                blocks.append(text)\n",
        "    return \"\\n\\n\".join(blocks)\n",
        "\n",
        "def load_text_from_txt(path: str) -> str:\n",
        "    \"\"\"Đọc text thuần từ file .txt.\"\"\"\n",
        "    try:\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Không đọc được TXT '{path}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def load_documents_from_folder(folder: str) -> List[Tuple[int, str]]:\n",
        "    \"\"\"\n",
        "    Quét thư mục, đọc các file .pdf và .txt.\n",
        "    Trả về list các tuple (doc_id, text_content). doc_id: 0,1,2,...\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    doc_id = 0\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for fname in files:\n",
        "            path = os.path.join(root, fname)\n",
        "            text = \"\"\n",
        "            if fname.lower().endswith(\".pdf\"):\n",
        "                text = load_texts_from_pdf(path)\n",
        "            elif fname.lower().endswith(\".txt\"):\n",
        "                text = load_text_from_txt(path)\n",
        "            else:\n",
        "                continue\n",
        "            if text and text.strip():\n",
        "                docs.append((doc_id, text))\n",
        "                logging.info(f\"[Load] doc_id={doc_id}, file='{path}', length={len(text)} chars\")\n",
        "                doc_id += 1\n",
        "            else:\n",
        "                logging.info(f\"[Skip] file '{path}' không có nội dung hoặc đọc lỗi.\")\n",
        "    if not docs:\n",
        "        logging.warning(\"[Warning] Không tìm thấy tài liệu hợp lệ trong thư mục.\")\n",
        "    return docs\n",
        "\n",
        "# ---------------------------------------\n",
        "# 4. Chunking cải tiến\n",
        "# Sử dụng tiktoken để đếm token\n",
        "_enc = None\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    global _enc\n",
        "    if _enc is None:\n",
        "        try:\n",
        "            _enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        except Exception:\n",
        "            try:\n",
        "                _enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "            except Exception:\n",
        "                _enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    try:\n",
        "        return len(_enc.encode(text))\n",
        "    except Exception:\n",
        "        return len(text.split())\n",
        "\n",
        "def chunk_text_langchain(text: str, max_tokens: int = 500, overlap_tokens: int = 50) -> list:\n",
        "    \"\"\"\n",
        "    Chunking sử dụng LangChain RecursiveCharacterTextSplitter dựa trên số token.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=max_tokens,\n",
        "        chunk_overlap=overlap_tokens,\n",
        "        length_function=count_tokens\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def chunk_text_langchain_with_table(text: str, max_tokens: int = 500, overlap_tokens: int = 50) -> list:\n",
        "    \"\"\"\n",
        "    Chunking sử dụng LangChain, nhưng bảng ([TABLE]) sẽ được chunk riêng biệt.\n",
        "    - Nếu là block bảng: chunk từng bảng riêng (có thể chia nhỏ theo dòng hoặc theo token)\n",
        "    - Nếu là block thường: chunk như cũ\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=max_tokens,\n",
        "        chunk_overlap=overlap_tokens,\n",
        "        length_function=count_tokens\n",
        "    )\n",
        "    chunks = []\n",
        "    blocks = text.split('\\n\\n')\n",
        "    for block in blocks:\n",
        "        block = block.strip()\n",
        "        if not block:\n",
        "            continue\n",
        "        if block.startswith('[TABLE]'):\n",
        "            # Chunk bảng: chia nhỏ theo dòng hoặc theo token\n",
        "            table_content = block[len('[TABLE]'):].strip()\n",
        "            # Có thể chia theo dòng hoặc chunk theo token, ở đây chunk theo token\n",
        "            table_chunks = splitter.split_text(table_content)\n",
        "            # Gắn lại prefix [TABLE] cho mỗi chunk bảng\n",
        "            for tchunk in table_chunks:\n",
        "                chunks.append('[TABLE]\\n' + tchunk)\n",
        "        else:\n",
        "            # Chunk đoạn thường\n",
        "            chunks.extend(splitter.split_text(block))\n",
        "    return chunks\n",
        "\n",
        "# ---------------------------------------\n",
        "# 5. Embedding với Azure OpenAI\n",
        "def get_embeddings(texts: List[str], batch_size: int = 20) -> List[List[float]]:\n",
        "    embeddings: List[List[float]] = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        try:\n",
        "            responses = client.embeddings.create(\n",
        "                model=EMBED_DEPLOYMENT,\n",
        "                input=batch\n",
        "            )\n",
        "            for data in responses.data:\n",
        "                embeddings.append(data.embedding)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lấy embedding batch lỗi: {e}. Thử lại sau 5s.\")\n",
        "            time.sleep(5)\n",
        "            responses = client.embeddings.create(\n",
        "                model=EMBED_DEPLOYMENT,\n",
        "                input=batch\n",
        "            )\n",
        "            for data in responses.data:\n",
        "                embeddings.append(data.embedding)\n",
        "        time.sleep(0.1)\n",
        "    return embeddings\n",
        "\n",
        "def normalize_vector(vec: List[float]) -> List[float]:\n",
        "    arr = np.array(vec, dtype=np.float32)\n",
        "    norm = np.linalg.norm(arr)\n",
        "    if norm == 0:\n",
        "        return arr.tolist()\n",
        "    return (arr / norm).tolist()\n",
        "\n",
        "def insert_chunks_to_milvus(collection: Collection, doc_id: int, chunks: List[str], batch_size: int = 20):\n",
        "    n = len(chunks)\n",
        "    chunk_ids = list(range(n))\n",
        "    for i in range(0, n, batch_size):\n",
        "        batch_chunks = chunks[i:i+batch_size]\n",
        "        batch_chunk_ids = chunk_ids[i:i+batch_size]\n",
        "        embs = get_embeddings(batch_chunks, batch_size=batch_size)\n",
        "        embs_norm = [normalize_vector(e) for e in embs]\n",
        "        doc_ids = [doc_id] * len(batch_chunks)\n",
        "        texts = batch_chunks\n",
        "        # Order: embedding, doc_id, chunk_id, text\n",
        "        entities = [\n",
        "            embs_norm,\n",
        "            doc_ids,\n",
        "            batch_chunk_ids,\n",
        "            texts\n",
        "        ]\n",
        "        try:\n",
        "            collection.insert(entities)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Insert vào Milvus lỗi: {e}\")\n",
        "        time.sleep(0.05)\n",
        "    collection.flush()\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Search và chat\n",
        "def embed_query(query: str) -> List[float]:\n",
        "    try:\n",
        "        response = client.embeddings.create(\n",
        "            model=EMBED_DEPLOYMENT,\n",
        "            input=[query]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"embed_query lỗi: {e}. Thử lại sau 2s.\")\n",
        "        time.sleep(2)\n",
        "        response = client.embeddings.create(\n",
        "            model=EMBED_DEPLOYMENT,\n",
        "            input=[query]\n",
        "        )\n",
        "    vec = response.data[0].embedding\n",
        "    return normalize_vector(vec)\n",
        "\n",
        "def search_milvus(collection: Collection, query_embedding: List[float], top_k: int = 30) -> List[Tuple[str, float]]:\n",
        "    results = collection.search(\n",
        "        data=[query_embedding],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}},\n",
        "        limit=top_k,\n",
        "        output_fields=[\"doc_id\", \"chunk_id\", \"text\"]\n",
        "    )\n",
        "    hits = results[0]\n",
        "    contexts: List[Tuple[str, float]] = []\n",
        "    for hit in hits:\n",
        "        txt = hit.entity.get(\"text\")\n",
        "        score = hit.score\n",
        "        contexts.append((txt, score))\n",
        "    return contexts\n",
        "\n",
        "def build_prompt(contexts: List[Tuple[str, float]], question: str, max_context_tokens: int = 1500) -> List[dict]:\n",
        "    contexts_sorted = sorted(contexts, key=lambda x: x[1], reverse=True)\n",
        "    try:\n",
        "        encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    except Exception:\n",
        "        encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    selected = []\n",
        "    total_tokens = 0\n",
        "    for text, score in contexts_sorted:\n",
        "        toks = len(encoder.encode(text))\n",
        "        if total_tokens + toks > max_context_tokens:\n",
        "            continue\n",
        "        selected.append(text)\n",
        "        total_tokens += toks\n",
        "    context_text = \"\\n---\\n\".join(selected) if selected else \"\"\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"Bạn là trợ lý nội bộ. Sử dụng chỉ thông tin nội bộ được cung cấp ở phần 'context' nếu có liên quan \"\n",
        "            \"để trả lời câu hỏi. Nếu không tìm thấy thông tin đủ, hãy trả lời trung thực là không biết.\"\n",
        "        )\n",
        "    }\n",
        "    if context_text:\n",
        "        user_content = f\"Dữ liệu tham khảo:\\n{context_text}\\n\\nCâu hỏi: {question}\"\n",
        "    else:\n",
        "        user_content = f\"Câu hỏi: {question}\\n(Lưu ý: Không tìm thấy thông tin liên quan trong knowledge base.)\"\n",
        "    user_message = {\"role\": \"user\", \"content\": user_content}\n",
        "    return [system_message, user_message]\n",
        "\n",
        "def get_answer_from_azure(messages: List[dict], max_tokens: int = 512, temperature: float = 0.2) -> str:\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHAT_DEPLOYMENT,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"ChatCompletion lỗi: {e}. Thử lại sau 2s.\")\n",
        "        time.sleep(2)\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHAT_DEPLOYMENT,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    answer = response.choices[0].message.content\n",
        "    return answer\n",
        "\n",
        "def answer_query(collection: Collection, question: str, top_k: int = 30) -> str:\n",
        "    q_emb = embed_query(question)\n",
        "    contexts = search_milvus(collection, q_emb, top_k=top_k)\n",
        "    low = question.strip().lower()\n",
        "    greetings = {\"hi\", \"hello\", \"chào\", \"xin chào\", \"hey\"}\n",
        "    if low in greetings or any(low.startswith(g) for g in greetings):\n",
        "        return \"Chào bạn! Bạn có thể hỏi tôi điều gì về nội dung đã được cung cấp.\"\n",
        "    messages = build_prompt(contexts, question)\n",
        "    answer = get_answer_from_azure(messages)\n",
        "    return answer\n",
        "\n",
        "# ---------------------------------------\n",
        "# 7. CLI hoặc API\n",
        "def interactive_cli(collection: Collection):\n",
        "    logging.info(\"=== ChatBot RAG (Azure OpenAI + Milvus) ===\")\n",
        "    logging.info(\"Nhập câu hỏi của bạn (gõ 'exit' hoặc 'quit' để dừng):\")\n",
        "    # chat_history = []  # Lưu lịch sử hội thoại\n",
        "    # max_history = 10   # Số lượt hội thoại gần nhất giữ lại (có thể điều chỉnh)\n",
        "    while True:\n",
        "        try:\n",
        "            question = input(\"Bạn hỏi: \").strip()\n",
        "        except (KeyboardInterrupt, EOFError):\n",
        "            logging.info(\"Thoát.\")\n",
        "            break\n",
        "        if not question:\n",
        "            continue\n",
        "        if question.lower() in {\"exit\", \"quit\"}:\n",
        "            logging.info(\"Thoát.\")\n",
        "            break\n",
        "        # Lấy context từ Milvus như cũ\n",
        "        q_emb = embed_query(question)\n",
        "        contexts = search_milvus(collection, q_emb, top_k=30)\n",
        "        messages = build_prompt(contexts, question)\n",
        "        # Nối lịch sử hội thoại vào sau messages (trừ câu hỏi hiện tại)\n",
        "        # Chỉ lấy max_history lượt gần nhất để tránh quá dài\n",
        "        # trimmed_history = chat_history[-max_history*2:]  # mỗi lượt gồm user+assistant\n",
        "        # full_messages = messages + trimmed_history\n",
        "        ans = get_answer_from_azure(messages)\n",
        "        print(\"Bot trả lời:\", ans)\n",
        "        print(\"-\" * 40)\n",
        "        # Lưu vào lịch sử\n",
        "        # chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "        # chat_history.append({\"role\": \"assistant\", \"content\": ans})\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"RAG ChatBot with Azure OpenAI + Milvus\")\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        choices=[\"index\", \"chat\", \"index_and_chat\"],\n",
        "        default=\"index_and_chat\",\n",
        "        help=\"Chế độ: 'index' chỉ indexing, 'chat' chỉ chat (giả định đã index), 'index_and_chat' làm cả hai.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--docs_folder\",\n",
        "        type=str,\n",
        "        default=\"./docs\",\n",
        "        help=\"Thư mục chứa tài liệu để index (pdf, txt).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--collection_name\",\n",
        "        type=str,\n",
        "        default=\"docsEngLC21\",\n",
        "        help=\"Tên collection Milvus.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dim\",\n",
        "        type=int,\n",
        "        default=1536,\n",
        "        help=\"Dimension embedding (ví dụ 1536 cho text-embedding-ada-002).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top_k\",\n",
        "        type=int,\n",
        "        default=30,\n",
        "        help=\"Số top chunks lấy từ Milvus khi search.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_tokens\",\n",
        "        type=int,\n",
        "        default=500,\n",
        "        help=\"Giới hạn token mỗi chunk khi indexing.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overlap_tokens\",\n",
        "        type=int,\n",
        "        default=50,\n",
        "        help=\"Số token overlap giữa các chunk khi indexing.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--verbose_chunk\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Nếu bật, sẽ in thông tin chi tiết khi chunking.\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    connect_milvus()\n",
        "\n",
        "    collection = None\n",
        "    if args.mode in {\"index\", \"index_and_chat\"}:\n",
        "        collection = create_collection(args.collection_name, args.dim)\n",
        "        docs = load_documents_from_folder(args.docs_folder)\n",
        "        if not docs:\n",
        "            logging.error(\"Không có tài liệu để index. Kiểm tra lại thư mục.\")\n",
        "            sys.exit(1)\n",
        "        for doc_id, text in docs:\n",
        "            logging.info(f\"[Indexing] doc_id={doc_id}, chunking ...\")\n",
        "            chunks = chunk_text_langchain_with_table(\n",
        "                text,\n",
        "                max_tokens=args.max_tokens,\n",
        "                overlap_tokens=args.overlap_tokens\n",
        "            )\n",
        "            logging.info(f\"[Indexing] doc_id={doc_id}, {len(chunks)} chunks\")\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                print(f\"--- Chunk {i} ---\")\n",
        "                print(chunk)\n",
        "                print(\"----------------\")\n",
        "            insert_chunks_to_milvus(collection, doc_id, chunks, batch_size=20)\n",
        "            logging.info(f\"[Indexing] doc_id={doc_id} hoàn thành.\")\n",
        "        logging.info(\"[Indexing] Hoàn thành indexing toàn bộ tài liệu.\")\n",
        "    if args.mode in {\"chat\", \"index_and_chat\"}:\n",
        "        if collection is None:\n",
        "            if args.collection_name not in [c.name for c in utility.list_collections()]:\n",
        "                logging.error(f\"Collection '{args.collection_name}' không tồn tại. Hãy chạy với --mode index trước.\")\n",
        "                sys.exit(1)\n",
        "            collection = Collection(args.collection_name)\n",
        "            collection.load()\n",
        "            logging.info(f\"[Milvus] Loaded existing collection '{args.collection_name}'.\")\n",
        "        interactive_cli(collection)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}